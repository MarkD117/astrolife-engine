# Hackumentary

## 404 - Idea not found
On day one of our SpaceApps journey we attempted to build a Space Biology Knowledge Engine, hosted on a webpage, utilisng AI that we had trained on Data provided by NASA. Unfortunately, our collective knowledge gaps proved too much to acheive a viable prototype and we have had to pivot into a new challenge.

This has given us a valuable learning experience both in adapting our approach from concept to build and given us an insight into where we can direct future learning. It is however, not a total failure, as we have gained an experience and also, througout our attempt we documented everything, with text, imagery and video, we met many amazing fellow hackers, we collaborated and learned.

## ctrl-alt-pivot
Which leads us to our new challenge, Hackumentary will be a documentary of sorts, hosted on a webpage of our making, will feature our process for our first attempt, testimonials, video interviews both with the team and fellow hackers and imagery.

## astrolife-engine
A web application that leverages AI, knowledge graphs, and/or other tools to summarize the 608 NASA bioscience publications listed in an online repository, and enables users to explore the impacts and results from the experiments those publications describe.

## Index - Table of contents
* [Tasks](#tasks)
* [Reference Links](#reference-links)
* placeholder

## Tasks
# AstroLife Engine

<p align="center">
    <img src="Documentation/Images/1000032404.jpg"/ height=400px>
</p>

### Google Cloud Platform Team: Dylan/Ciara
Enabled APIs
Vertex AI API - To train machine learning modules.
Cloud Storage API - Storing the NASA Repository (.CSV files).
Roadblocks hit. AI learning would not complete, several different changes attempted but utimately beyond our skillset. 

Created Bucket
Downloaded the 608 publication links from the NASA resource repository in .CSV format and added them to the bucket on GCP.

Trained AI Model
Imported the .CSV files from the bucket to Vertex AI and trained the AI model.

### GoDaddy Domain/Website Build Team: Mark/Rob
Chose a .study domain as other options didn’t allow for WHOIS security.
First major roadblacks hit, previously used software no longer available 

### Graph Integration Team: Alex/George
Researching methods for integrating graphs/ taking graph data from repositories.

### Documentation: All/Rob

# Hackumentary
<p align="center">
    <img src="Documentation/Images/1000032430.jpg"/ height=400px>
</p>

### Website Build Team: Mark/George
We chose a .study domain as other options didn’t allow for WHOIS security.
We used the GoDaddy website builder, we had considered building our own from scratch in VS Code, but given we had already lost our first day we opted for a website builder to be as efficent as possible with our remaining time.
A privacy policy was generated

### Media Editing Team: Ciara/Dylan
As the content creation team captured footage and images, they would send this to our editing team via WhatsApp and email. The editing team would then sort through the content to find what best suited our needs and using DaVinci Resolve would edit the raw footage.

### Content Creation Team: Alex
Using Alex's iPad, we continued to take video of our team throughout our creation process. We also took video of other hackers participating, with their consent and interviewed willing teams with their projects and HackAthlone staff and volunteers on site for their perspective and experiences. 

### Documentation: Rob
Each individual kept a timeline of their work, Rob then gathered this and kept updating the ReadME file on github. 

## Reference Links






